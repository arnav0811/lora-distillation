# lora-distillation
Implement LoRA adapters on a 7B Mistral Model and distill into a smaller student
